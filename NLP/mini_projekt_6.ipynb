{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7973/294544994.py:13: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "# sns.set()\n",
    "\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "# import torchvision\n",
    "# from torchvision.utils import save_image\n",
    "# from torchvision.datasets import FashionMNIST\n",
    "# from torchvision.transforms import v2\n",
    "# from torchvision.datasets import ImageFolder\n",
    "# from torch.nn.functional import one_hot\n",
    "\n",
    "# from pytorch_fid import fid_score\n",
    "\n",
    "# from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import *\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wczytywanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>location not palace excellent hotel booke dthe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>respite definitely not place stay looking ultr...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stunning truly memorable spot right beach nusa...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>solid business hotel near embassy stayed hotel...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nice place make sure lock money warning money ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating\n",
       "0  location not palace excellent hotel booke dthe...       4\n",
       "1  respite definitely not place stay looking ultr...       3\n",
       "2  stunning truly memorable spot right beach nusa...       4\n",
       "3  solid business hotel near embassy stayed hotel...       3\n",
       "4  nice place make sure lock money warning money ...       3"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = '../../data'\n",
    "reviews = pd.read_csv(f\"train_data.csv\")\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "4    7243\n",
       "3    4831\n",
       "2    1747\n",
       "1    1434\n",
       "0    1137\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z komórki wyżej wynika, że dane treningowe są niezbilansowane. \\\n",
    "Znamy 2 główne metody \"naprawy\" tego problemu: undersampling i oversampling. \\\n",
    "Zdecydowaliśmy się zastosować metodę undersamplingu, w nadziei, że prawie 1200 wierszy dla każdej z klas wystarczy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reviews.filter(['review'])\n",
    "y = reviews.filter(['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = RandomUnderSampler(sampling_strategy=\"not minority\")\n",
    "resampled_train_data, resampled_train_classes = sampler.fit_resample(X_train, y_train)\n",
    "resampled_test_data, resampled_test_classes = sampler.fit_resample(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "0         790\n",
       "1         790\n",
       "2         790\n",
       "3         790\n",
       "4         790\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_train_classes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "0         347\n",
       "1         347\n",
       "2         347\n",
       "3         347\n",
       "4         347\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_test_classes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "Łącznie, resampled_train_classes oraz resampled_test_classes mają po 1137 w każdej z klas. \\\n",
    "\\\n",
    "Najpierw podzieliliśmy początkowy zbiór testowy na nowe dane testowe i walidacyjne, a następnie zbilansowaliśmy dane testowe otrzymane w wyniku podziału. Kolejność tych operacji jest ważna, ponieważ chcemy, aby zbiór walidacyjny nie zawierał żadnych zmian (aby zbiór walidacyjny był jak najbliższy \"rzeczywistym\" danym)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled = pd.merge(resampled_train_data, resampled_train_classes, left_index=True, right_index=True)\n",
    "resampled_test = pd.merge(resampled_test_data, resampled_test_classes, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4635</th>\n",
       "      <td>disappointed service there-the restaurant open...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13177</th>\n",
       "      <td>run thought picking cheap hotel save lot, not,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5778</th>\n",
       "      <td>rating suspect, hotel sofitel june 6-7 power w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3526</th>\n",
       "      <td>words, ok stay riu hotel punta cana, twice sta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>n't fooled price location grade room damrak ho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  rating\n",
       "4635   disappointed service there-the restaurant open...       0\n",
       "13177  run thought picking cheap hotel save lot, not,...       0\n",
       "5778   rating suspect, hotel sofitel june 6-7 power w...       0\n",
       "3526   words, ok stay riu hotel punta cana, twice sta...       0\n",
       "982    n't fooled price location grade room damrak ho...       0"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "0    790\n",
       "1    790\n",
       "2    790\n",
       "3    790\n",
       "4    790\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Rekurencyjny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "corpus = api.load('text8')\n",
    "gensim_model = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "class LSTMRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, out_size, emb_weights, bidirectional = False):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        if bidirectional:\n",
    "            self.bidirectional = 2\n",
    "        else:\n",
    "            self.bidirectional = 1\n",
    "        self.embeddings = nn.Embedding.from_pretrained(emb_weights)\n",
    "        self.embeddings.requires_grad = False\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, bidirectional=bidirectional, batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_size*self.bidirectional, out_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
    "        state = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
    "        return hidden, state\n",
    "\n",
    "    def forward(self, x, len_x, hidden):\n",
    "        x = self.embeddings(x)\n",
    "        x = torch.transpose(x,0,1)\n",
    "        all_outputs, hidden = self.lstm(x, hidden)\n",
    "        all_outputs = torch.transpose(all_outputs,0,1)\n",
    "        last_seq_items = all_outputs[range(all_outputs.shape[0]), len_x]\n",
    "        out = last_seq_items\n",
    "        x = self.fc(out)\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_weights = torch.FloatTensor(gensim_model.wv.vectors)\n",
    "embedding = nn.Embedding.from_pretrained(emb_weights)\n",
    "tokenizer = gensim_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMRegressor(\n",
       "  (embeddings): Embedding(71290, 100)\n",
       "  (lstm): LSTM(100, 100)\n",
       "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = LSTMRegressor(100, 100, 1, 5, emb_weights).to(device)\n",
    "lstm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby usprawnić działanie metody BoW, zastosujemy tzw. \"stopwords\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/michal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Data cleaning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words(raw_review):\n",
    "    \"\"\"Function to convert a review to a string of words.\n",
    "    The input is a single string (a raw moviw review), and the output is a single string (a preprocessed movie review)\"\"\"\n",
    "    review_text = BeautifulSoup(raw_review, 'lxml').get_text()\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words('english'))\n",
    "    meaningful_words = [word for word in words if not word in stops]\n",
    "    return \" \".join(meaningful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_reviews(reviews):\n",
    "    clean_reviews = []\n",
    "    num_reviews = reviews['review'].size\n",
    "\n",
    "    for i, review in enumerate(reviews['review']):\n",
    "        if i % 200 == 0:\n",
    "            print('Review {} of {}'.format(i, num_reviews))\n",
    "        clean_reviews.append(review_to_words(review))\n",
    "    return clean_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 3950\n",
      "Review 200 of 3950\n",
      "Review 400 of 3950\n",
      "Review 600 of 3950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7973/3593606712.py:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  review_text = BeautifulSoup(raw_review, 'lxml').get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 800 of 3950\n",
      "Review 1000 of 3950\n",
      "Review 1200 of 3950\n",
      "Review 1400 of 3950\n",
      "Review 1600 of 3950\n",
      "Review 1800 of 3950\n",
      "Review 2000 of 3950\n",
      "Review 2200 of 3950\n",
      "Review 2400 of 3950\n",
      "Review 2600 of 3950\n",
      "Review 2800 of 3950\n",
      "Review 3000 of 3950\n",
      "Review 3200 of 3950\n",
      "Review 3400 of 3950\n",
      "Review 3600 of 3950\n",
      "Review 3800 of 3950\n",
      "Review 0 of 1735\n",
      "Review 200 of 1735\n",
      "Review 400 of 1735\n",
      "Review 600 of 1735\n",
      "Review 800 of 1735\n",
      "Review 1000 of 1735\n",
      "Review 1200 of 1735\n",
      "Review 1400 of 1735\n",
      "Review 1600 of 1735\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "clean_reviews_train = []\n",
    "num_reviews = reviews['review'].size\n",
    "\n",
    "clean_reviews_train = get_clean_reviews(resampled)\n",
    "clean_reviews_test = get_clean_reviews(resampled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "Bag of words completed\n"
     ]
    }
   ],
   "source": [
    "print('Creating the bag of words...')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                            tokenizer = None,\n",
    "                            preprocessor = None,\n",
    "                            stop_words = None,\n",
    "                            max_features = 3000)\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_reviews_train)\n",
    "valid_data_features = vectorizer.fit_transform(clean_reviews_test)\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n",
    "valid_data_features = valid_data_features.toarray()\n",
    "print('Bag of words completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_train_classes = resampled_train_classes.to_numpy().flatten()\n",
    "flatten_test_classes = resampled_test_classes.to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.random.rand(len(reviews))>0.3\n",
    "train_data = torch.from_numpy(train_data_features).float()\n",
    "train_targets = torch.from_numpy(resampled['rating'].values).long()\n",
    "\n",
    "test_data = torch.from_numpy(valid_data_features).float()\n",
    "test_targets = torch.from_numpy(resampled_test['rating'].values).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = torch.from_numpy(train_data_features).float()\n",
    "# train_targets = torch.from_numpy(flatten_train_classes).long()\n",
    "\n",
    "# test_data ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(train_data, train_targets)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_data, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoWClassifier(\n",
       "  (lin1): Linear(in_features=3000, out_features=512, bias=True)\n",
       "  (d1): Dropout1d(p=0.3, inplace=False)\n",
       "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): LeakyReLU(negative_slope=0.01)\n",
       "  (lin2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (d2): Dropout1d(p=0.3, inplace=False)\n",
       "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act2): LeakyReLU(negative_slope=0.01)\n",
       "  (lin3): Linear(in_features=256, out_features=124, bias=True)\n",
       "  (d3): Dropout1d(p=0.3, inplace=False)\n",
       "  (bn3): BatchNorm1d(124, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act3): LeakyReLU(negative_slope=0.01)\n",
       "  (lin4): Linear(in_features=124, out_features=64, bias=True)\n",
       "  (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act4): LeakyReLU(negative_slope=0.01)\n",
       "  (d4): Dropout1d(p=0.3, inplace=False)\n",
       "  (act5): LeakyReLU(negative_slope=0.01)\n",
       "  (lin5): Linear(in_features=64, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.lin1 =nn.Linear(3000, 512)\n",
    "        self.d1 = nn.Dropout1d(0.3)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.act1 =nn.LeakyReLU()\n",
    "\n",
    "        self.lin2 =nn.Linear(512, 256)\n",
    "        self.d2 = nn.Dropout1d(0.3)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.act2 =nn.LeakyReLU()\n",
    "\n",
    "        self.lin3 = nn.Linear(256, 124)\n",
    "        self.d3 = nn.Dropout1d(0.3)\n",
    "        self.bn3 = nn.BatchNorm1d(124)\n",
    "        self.act3 = nn.LeakyReLU()\n",
    "\n",
    "        self.lin4 = nn.Linear(124, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.act4 = nn.LeakyReLU()\n",
    "        self.d4 = nn.Dropout1d(0.3)\n",
    "\n",
    "        self.act5 = nn.LeakyReLU()\n",
    "        self.lin5 =nn.Linear(64, 5)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.d1(x)\n",
    "\n",
    "        x = self.lin2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.d2(x)\n",
    "\n",
    "        x = self.lin3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.d3(x)\n",
    "\n",
    "        x = self.lin4(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.d4(x)\n",
    "\n",
    "        x = self.lin5(x)\n",
    "        return x\n",
    "bow_model = BoWClassifier().to(device)\n",
    "bow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval() #*********#\n",
    "    for x, labels in data_loader:\n",
    "        x, labels = x.to(device), labels.to(device)\n",
    "        output = model(x)\n",
    "        pred = output.max(1, keepdim=True)[1] # get the index of the max logit\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += x.shape[0]\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 1.26 train_acc: 0.974 valid_acc 0.20576368876080692\n",
      "Epoch 1 loss 1.25 train_acc: 0.977 valid_acc 0.21037463976945245\n",
      "Epoch 2 loss 1.25 train_acc: 0.977 valid_acc 0.19711815561959653\n",
      "Epoch 3 loss 1.25 train_acc: 0.984 valid_acc 0.19827089337175793\n",
      "Epoch 4 loss 1.25 train_acc: 0.985 valid_acc 0.2\n",
      "Epoch 5 loss 1.25 train_acc: 0.985 valid_acc 0.20576368876080692\n",
      "Epoch 6 loss 1.25 train_acc: 0.987 valid_acc 0.20403458213256484\n",
      "Epoch 7 loss 1.25 train_acc: 0.986 valid_acc 0.20576368876080692\n",
      "Epoch 8 loss 1.24 train_acc: 0.987 valid_acc 0.21037463976945245\n",
      "Epoch 9 loss 1.25 train_acc: 0.988 valid_acc 0.21210374639769453\n",
      "Epoch 10 loss 1.25 train_acc: 0.989 valid_acc 0.21440922190201728\n",
      "Epoch 11 loss 1.24 train_acc: 0.991 valid_acc 0.22420749279538904\n",
      "Epoch 12 loss 1.24 train_acc: 0.99 valid_acc 0.2201729106628242\n",
      "Epoch 13 loss 1.25 train_acc: 0.991 valid_acc 0.2132564841498559\n",
      "Epoch 14 loss 1.25 train_acc: 0.994 valid_acc 0.2195965417867435\n",
      "Epoch 15 loss 1.25 train_acc: 0.992 valid_acc 0.2132564841498559\n",
      "Epoch 16 loss 1.23 train_acc: 0.991 valid_acc 0.21786743515850143\n",
      "Epoch 17 loss 1.22 train_acc: 0.995 valid_acc 0.20576368876080692\n",
      "Epoch 18 loss 1.23 train_acc: 0.995 valid_acc 0.22536023054755044\n",
      "Epoch 19 loss 1.24 train_acc: 0.995 valid_acc 0.22478386167146974\n",
      "Epoch 20 loss 1.23 train_acc: 0.996 valid_acc 0.23688760806916426\n",
      "Epoch 21 loss 1.23 train_acc: 0.992 valid_acc 0.2219020172910663\n",
      "Epoch 22 loss 1.23 train_acc: 0.993 valid_acc 0.21671469740634006\n",
      "Epoch 23 loss 1.23 train_acc: 0.992 valid_acc 0.21613832853025935\n",
      "Epoch 24 loss 1.23 train_acc: 0.992 valid_acc 0.2219020172910663\n",
      "Epoch 25 loss 1.24 train_acc: 0.994 valid_acc 0.21556195965417868\n",
      "Epoch 26 loss 1.24 train_acc: 0.993 valid_acc 0.21786743515850143\n",
      "Epoch 27 loss 1.24 train_acc: 0.993 valid_acc 0.2270893371757925\n",
      "Epoch 28 loss 1.23 train_acc: 0.993 valid_acc 0.22305475504322766\n",
      "Epoch 29 loss 1.23 train_acc: 0.994 valid_acc 0.22074927953890489\n",
      "Epoch 30 loss 1.24 train_acc: 0.994 valid_acc 0.21095100864553315\n",
      "Epoch 31 loss 1.23 train_acc: 0.993 valid_acc 0.2132564841498559\n",
      "Epoch 32 loss 1.25 train_acc: 0.995 valid_acc 0.21613832853025935\n",
      "Epoch 33 loss 1.21 train_acc: 0.995 valid_acc 0.21844380403458213\n",
      "Epoch 34 loss 1.26 train_acc: 0.996 valid_acc 0.21613832853025935\n",
      "Epoch 35 loss 1.24 train_acc: 0.995 valid_acc 0.2213256484149856\n",
      "Epoch 36 loss 1.23 train_acc: 0.996 valid_acc 0.21786743515850143\n",
      "Epoch 37 loss 1.24 train_acc: 0.997 valid_acc 0.21440922190201728\n",
      "Epoch 38 loss 1.24 train_acc: 0.996 valid_acc 0.20864553314121037\n",
      "Epoch 39 loss 1.24 train_acc: 0.996 valid_acc 0.21268011527377523\n",
      "Epoch 40 loss 1.21 train_acc: 0.996 valid_acc 0.20288184438040346\n",
      "Epoch 41 loss 1.22 train_acc: 0.996 valid_acc 0.20634005763688762\n",
      "Epoch 42 loss 1.23 train_acc: 0.997 valid_acc 0.20288184438040346\n",
      "Epoch 43 loss 1.24 train_acc: 0.996 valid_acc 0.21440922190201728\n",
      "Epoch 44 loss 1.23 train_acc: 0.996 valid_acc 0.20979827089337175\n",
      "Epoch 45 loss 1.23 train_acc: 0.995 valid_acc 0.207492795389049\n",
      "Epoch 46 loss 1.23 train_acc: 0.997 valid_acc 0.20345821325648414\n",
      "Epoch 47 loss 1.23 train_acc: 0.996 valid_acc 0.20057636887608069\n",
      "Epoch 48 loss 1.23 train_acc: 0.996 valid_acc 0.20634005763688762\n",
      "Epoch 49 loss 1.23 train_acc: 0.997 valid_acc 0.20057636887608069\n",
      "Epoch 50 loss 1.24 train_acc: 0.997 valid_acc 0.2080691642651297\n",
      "Epoch 51 loss 1.23 train_acc: 0.997 valid_acc 0.20634005763688762\n",
      "Epoch 52 loss 1.22 train_acc: 0.996 valid_acc 0.20518731988472622\n",
      "Epoch 53 loss 1.24 train_acc: 0.997 valid_acc 0.21037463976945245\n",
      "Epoch 54 loss 1.23 train_acc: 0.998 valid_acc 0.20576368876080692\n",
      "Epoch 55 loss 1.24 train_acc: 0.997 valid_acc 0.20634005763688762\n",
      "Epoch 56 loss 1.25 train_acc: 0.998 valid_acc 0.21037463976945245\n",
      "Epoch 57 loss 1.24 train_acc: 0.998 valid_acc 0.21268011527377523\n",
      "Epoch 58 loss 1.23 train_acc: 0.997 valid_acc 0.21440922190201728\n",
      "Epoch 59 loss 1.24 train_acc: 0.998 valid_acc 0.2195965417867435\n",
      "Epoch 60 loss 1.23 train_acc: 0.996 valid_acc 0.21902017291066284\n",
      "Epoch 61 loss 1.23 train_acc: 0.996 valid_acc 0.21671469740634006\n",
      "Epoch 62 loss 1.22 train_acc: 0.998 valid_acc 0.21729106628242076\n",
      "Epoch 63 loss 1.21 train_acc: 0.997 valid_acc 0.22305475504322766\n",
      "Epoch 64 loss 1.22 train_acc: 0.995 valid_acc 0.2213256484149856\n",
      "Epoch 65 loss 1.21 train_acc: 0.997 valid_acc 0.21844380403458213\n",
      "Epoch 66 loss 1.25 train_acc: 0.997 valid_acc 0.21613832853025935\n",
      "Epoch 67 loss 1.22 train_acc: 0.997 valid_acc 0.2132564841498559\n",
      "Epoch 68 loss 1.21 train_acc: 0.998 valid_acc 0.22363112391930837\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[235], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m     epoch_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 18\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m loss_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(epoch_losses)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:439\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    437\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    441\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(bow_model.parameters(), lr=0.001)\n",
    "\n",
    "iters = []\n",
    "losses = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "for n in range(100):\n",
    "    epoch_losses = []\n",
    "    for x, labels in iter(train_loader):\n",
    "        x, labels = x.to(device), labels.to(device)\n",
    "        bow_model.train()\n",
    "        out = bow_model(x).squeeze()\n",
    "\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        epoch_losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    loss_mean = np.array(epoch_losses).mean()\n",
    "    iters.append(n)\n",
    "    losses.append(loss_mean)\n",
    "    test_acc = get_accuracy(bow_model, test_loader)\n",
    "    train_acc.append(get_accuracy(bow_model, train_loader)) # compute training accuracy\n",
    "    val_acc.append(test_acc)  # compute validation accuracy\n",
    "    print(f\"Epoch {n} loss {loss_mean:.3} train_acc: {train_acc[-1]:.3} valid_acc {test_acc}\")\n",
    "\n",
    "\n",
    "print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
    "print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gotowe modele z biblioteki sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineMultinomialNB = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('MNB', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipelineComplementNB = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('CNB', ComplementNB())\n",
    "])\n",
    "\n",
    "pipelineLinearSVM = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('LSVC', LinearSVC())\n",
    "])\n",
    "\n",
    "pipelineSVM = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('SVC', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(resampled_train_data))\n",
    "# print(type(resampled_train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(resampled_train_data))\n",
    "# print(type(resampled_train_classes))\n",
    "# print(resampled_train_data.shape)\n",
    "# print(resampled_train_classes.shape)\n",
    "# print(resampled_train_data.values.reshape(-1))\n",
    "# print(resampled_train_classes.values.reshape(-1))\n",
    "# print(resampled_train_data.values.shape)\n",
    "# print(resampled_train_classes.values.reshape(-1,1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5079300528670191\n"
     ]
    }
   ],
   "source": [
    "pipelineMultinomialNB.fit(resampled_train_data.values.flatten(), resampled_train_classes.values.flatten())\n",
    "predictMNB = pipelineMultinomialNB.predict(X_test.to_numpy().flatten())\n",
    "print(accuracy_score(y_test, predictMNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5453436356242375\n"
     ]
    }
   ],
   "source": [
    "pipelineComplementNB.fit(resampled_train_data.values.flatten(), resampled_train_classes.values.flatten())\n",
    "predictCNB = pipelineComplementNB.predict(X_test.to_numpy().flatten())\n",
    "print(accuracy_score(y_test, predictCNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5461569743798292\n"
     ]
    }
   ],
   "source": [
    "pipelineLinearSVM.fit(resampled_train_data.values.flatten(), resampled_train_classes.values.flatten())\n",
    "predictLSVM = pipelineLinearSVM.predict(X_test.to_numpy().flatten())\n",
    "print(accuracy_score(y_test, predictLSVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5673037820252135\n"
     ]
    }
   ],
   "source": [
    "pipelineSVM.fit(resampled_train_data.values.flatten(), resampled_train_classes.values.flatten())\n",
    "predictSVM = pipelineSVM.predict(X_test.to_numpy().flatten())\n",
    "print(accuracy_score(y_test, predictSVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sposród przetestowanych przez nas gotowych modeli dostarczonych w bibliotece scikit-learn, najlepsze wyniki daje model SVM. Zauważyliśmy przy tym, że przeprowadzenie całości ostatniego pipeline'u zajmuje znacznie więcej czasu (~25-30 sek.) w porównaniu do pozostałych trzech modeli (~0.7 sek.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele rekurencyjne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "corpus = api.load('text8')\n",
    "gensim_model = Word2Vec(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_weights = torch.FloatTensor(gensim_model.wv.vectors)\n",
    "embedding = nn.Embedding.from_pretrained(emb_weights)\n",
    "tokenizer = gensim_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5636"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer[\"aa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_reviews_tokenized = []\n",
    "for review in reviews['review']:\n",
    "    unknows = 0\n",
    "    all_parsed = 0\n",
    "    review_tokenized = []\n",
    "    for word in review.split():\n",
    "        all_parsed+=1\n",
    "        try:\n",
    "            review_tokenized.append(tokenizer[word.lower()])\n",
    "        except:\n",
    "            unknows +=1\n",
    "    clean_train_reviews_tokenized.append(review_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_reviews_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(data.Dataset):\n",
    "    def __init__(self, data,labels):\n",
    "        self.data = []\n",
    "        for d, l in zip(data,labels):\n",
    "            self.data.append((torch.from_numpy(np.array(d)).long(),torch.tensor(l).long()))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        in_data, target = self.data[idx]\n",
    "        return in_data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ..., False,  True, False])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_indices = np.random.rand(len(clean_train_reviews_tokenized))>0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ReviewDataset(np.array(clean_train_reviews_tokenized, dtype=object)[train_indices],reviews[\"rating\"].values[train_indices])\n",
    "test_data = ReviewDataset(np.array(clean_train_reviews_tokenized, dtype=object)[~train_indices],reviews[\"rating\"].values[~train_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    x_lens = [len(x)-1 for x in xx]\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=1)\n",
    "    yy = torch.stack(yy)\n",
    "    return xx_pad, yy, x_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_data, batch_size=32, collate_fn=pad_collate, shuffle=True,drop_last=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=32, collate_fn=pad_collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMRegressor(\n",
       "  (embeddings): Embedding(71290, 100)\n",
       "  (lstm): LSTM(100, 100)\n",
       "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "class LSTMRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, out_size, emb_weights, bidirectional = False):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        if bidirectional:\n",
    "            self.bidirectional = 2\n",
    "        else:\n",
    "            self.bidirectional = 1\n",
    "        self.embeddings = nn.Embedding.from_pretrained(emb_weights)\n",
    "        self.embeddings.requires_grad = False\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, bidirectional=bidirectional, batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_size*self.bidirectional, out_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
    "        state = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
    "        return hidden, state\n",
    "\n",
    "    def forward(self, x, len_x, hidden):\n",
    "        x = self.embeddings(x)\n",
    "        x = torch.transpose(x,0,1)\n",
    "        all_outputs, hidden = self.lstm(x, hidden)\n",
    "        all_outputs = torch.transpose(all_outputs,0,1)\n",
    "        last_seq_items = all_outputs[range(all_outputs.shape[0]), len_x]\n",
    "        out = last_seq_items\n",
    "        x = self.fc(out)\n",
    "        return x, hidden\n",
    "\n",
    "lstm_model = LSTMRegressor(100, 100, 1, 5, emb_weights).to(device)\n",
    "lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr = 0.001)\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "lstm_model.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(31):\n",
    "    losses = 0\n",
    "    batches = 0\n",
    "    for x, targets, len_x in train_loader:\n",
    "        x = x.to(device)\n",
    "        targets = targets.to(device)\n",
    "        hidden, state = lstm_model.init_hidden(x.size(0))\n",
    "        hidden, state = hidden.to(device), state.to(device)\n",
    "        preds, _ = lstm_model(x, len_x, (hidden,state))\n",
    "        preds = preds.squeeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fun(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        batches +=1\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, loss: {losses/batches:.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    for x, targets, len_x in test_loader:\n",
    "        x = x.to(device)\n",
    "        targets_list.append(targets.numpy())\n",
    "        targets = targets.to(device)\n",
    "        hidden, state = lstm_model.init_hidden(x.size(0))\n",
    "        hidden, state = hidden.to(device), state.to(device)\n",
    "        preds, _ = lstm_model(x, len_x, (hidden,state))\n",
    "        preds = preds.squeeze(1)\n",
    "        preds_list.append(preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.539\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test accuracy: {(np.argmax((np.concatenate(preds_list)),1) == np.concatenate(targets_list)).sum()/len(np.concatenate(targets_list)):.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_1_text = \"I do not like this hotel\"\n",
    "example_2_text = \"I like this hotel\"\n",
    "example_1_tokenized = []\n",
    "for word in example_1_text.split():\n",
    "    try:\n",
    "        example_1_tokenized.append(tokenizer[word])\n",
    "    except:\n",
    "        continue\n",
    "example_2_tokenized = []\n",
    "for word in example_2_text.split():\n",
    "    try:\n",
    "        example_2_tokenized.append(tokenizer[word])\n",
    "    except:\n",
    "        continue\n",
    "hidden, state = lstm_model.init_hidden(1)\n",
    "hidden, state = hidden.to(device), state.to(device)\n",
    "preds_1,_ = lstm_model(torch.from_numpy(np.array(example_1_tokenized)).unsqueeze(0).to(device),len(example_1_tokenized)-1,(hidden,state))\n",
    "preds_2,_ = lstm_model(torch.from_numpy(np.array(example_2_tokenized)).unsqueeze(0).to(device),len(example_2_tokenized)-1,(hidden,state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(preds_1.detach().numpy()))\n",
    "print(np.argmax(preds_2.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embbedings with bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(data.Dataset):\n",
    "    def __init__(self, data, mask, labels):\n",
    "        self.data = data\n",
    "        self.mask = mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        in_data = self.data[idx]\n",
    "        in_mask = self.mask[idx]\n",
    "        in_target = self.labels[idx]\n",
    "        return in_data, in_mask, in_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_two = tokenizer(reviews['review'].to_numpy().tolist(), return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs_two['input_ids']\n",
    "attention_mask = inputs_two['attention_mask']\n",
    "train_indices = np.random.rand(len(inputs))>0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ReviewDataset(inputs[train_indices], attention_mask[train_indices], reviews[\"rating\"].values[train_indices])\n",
    "test_data = ReviewDataset(inputs[~train_indices], attention_mask[~train_indices], reviews[\"rating\"].values[~train_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    x_lens = [len(x)-1 for x in xx]\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=1)\n",
    "    yy = torch.stack(yy)\n",
    "    return xx_pad, yy, x_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_data, batch_size=128, shuffle=True,drop_last=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=128,  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMRegressor(\n",
       "  (bert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lstm): LSTM(100, 100)\n",
       "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "class LSTMRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, out_size, bert, bidirectional = False):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        if bidirectional:\n",
    "            self.bidirectional = 2\n",
    "        else:\n",
    "            self.bidirectional = 1\n",
    "        self.bert = bert\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, bidirectional=bidirectional, batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_size*self.bidirectional, out_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
    "        state = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
    "        return hidden, state\n",
    "\n",
    "    def forward(self, x, attention_mask, hidden):\n",
    "        x = self.bert(input_ids=x, attention_mask=attention_mask)\n",
    "        x = torch.transpose(x,0,1)\n",
    "        all_outputs, hidden = self.lstm(x, hidden)\n",
    "        all_outputs = torch.transpose(all_outputs,0,1)\n",
    "        last_seq_items = all_outputs[range(all_outputs.shape[0]), len_x]\n",
    "        out = last_seq_items\n",
    "        x = self.fc(out)\n",
    "        return x, hidden\n",
    "\n",
    "lstm_model = LSTMRegressor(100, 100, 1, 5, bert=model).to(device)\n",
    "lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr = 0.001)\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "lstm_model.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(31):\n",
    "    losses = 0\n",
    "    batches = 0\n",
    "    for x, mask, targets in train_loader:\n",
    "        x = x.to(device)\n",
    "        targets = targets.to(device)\n",
    "        hidden, state = lstm_model.init_hidden(x.size(0))\n",
    "        hidden, state = hidden.to(device), state.to(device)\n",
    "        preds, _ = lstm_model(x, mask, (hidden, state))\n",
    "        preds = preds.squeeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fun(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        batches +=1\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, loss: {losses/batches:.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    for x, targets, len_x in test_loader:\n",
    "        x = x.to(device)\n",
    "        targets_list.append(targets.numpy())\n",
    "        targets = targets.to(device)\n",
    "        hidden, state = lstm_model.init_hidden(x.size(0))\n",
    "        hidden, state = hidden.to(device), state.to(device)\n",
    "        preds, _ = lstm_model(x, len_x, (hidden,state))\n",
    "        preds = preds.squeeze(1)\n",
    "        preds_list.append(preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.539\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test accuracy: {(np.argmax((np.concatenate(preds_list)),1) == np.concatenate(targets_list)).sum()/len(np.concatenate(targets_list)):.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_1_text = \"I do not like this hotel\"\n",
    "example_2_text = \"I like this hotel\"\n",
    "example_1_tokenized = []\n",
    "for word in example_1_text.split():\n",
    "    try:\n",
    "        example_1_tokenized.append(tokenizer[word])\n",
    "    except:\n",
    "        continue\n",
    "example_2_tokenized = []\n",
    "for word in example_2_text.split():\n",
    "    try:\n",
    "        example_2_tokenized.append(tokenizer[word])\n",
    "    except:\n",
    "        continue\n",
    "hidden, state = lstm_model.init_hidden(1)\n",
    "hidden, state = hidden.to(device), state.to(device)\n",
    "preds_1,_ = lstm_model(torch.from_numpy(np.array(example_1_tokenized)).unsqueeze(0).to(device),len(example_1_tokenized)-1,(hidden,state))\n",
    "preds_2,_ = lstm_model(torch.from_numpy(np.array(example_2_tokenized)).unsqueeze(0).to(device),len(example_2_tokenized)-1,(hidden,state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(preds_1.detach().numpy()))\n",
    "print(np.argmax(preds_2.detach().numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
